{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Networks w/ MNIST\n",
    "\n",
    "LSTMS are a special kind of Recurrent Neural Network capable of learning long-term dependencies. An appeal of RNNs is the idea that they would be able to use previous knowledge for the current task at hand, and in theory RNNs should be able to handle even large gaps between past information and current predictions.\n",
    "\n",
    "LSTMS use a cell state to achieve long-term dependencies. All RNNs have a repeating module, usually a tanh layer, that is combined with the current network from a previous network that receives and outputs the current prediction as well as the recurrent input for the next neural network. \n",
    "\n",
    "here are some images found online courtesy of http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "\n",
    "![](lstm1.png)\n",
    "\n",
    "\n",
    "![](lstm2.png)\n",
    "\n",
    "\n",
    "![](lstm3.png)\n",
    "\n",
    "These images detail a general overview of what is happening in a LSTM. There are four neural network layers that interact in a unique way detailed specifically here.\n",
    "\n",
    "![](lstm4.png)\n",
    "\n",
    "The first step of our network is to set up the cell state initially using a sigmoid later called the \"forget gate layer\"\n",
    "\n",
    "![](lstm5.png)\n",
    "\n",
    "The enxt step is to decide the information stored in the cell state. The sigmoid layer is the input gate layer and the tanh layer creates a vector of new candidates values C_t, that could be added to the state.\n",
    "\n",
    "\n",
    "![](lstm6.png)\n",
    "\n",
    "Then we update the old cell State C_t-1 into the new cell state, C_t. We mulitple our old state by f_t and then we add i_t * C_t. These become the new candidate values.\n",
    "\n",
    "![](lstm7.png)\n",
    "\n",
    "Finally we decide what we're going to output. This output will be based on our cell state, but will be a filtered version, with a sigmoid layer that decides the cell state, the tanh layer and then mulitpled by the sigmoid gate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is how we trained our model using the described optimier. We also added logging to every 100th iteration in the training process to understand how our training accuracy and average loss is going. Our final Testing accruacy was : 0.976562.\n",
    "\n",
    "Here are some outputs from tensorflow and relevant graphs\n",
    "\n",
    "\n",
    "![Graph Model](better_rnn.png)\n",
    "\n",
    "\n",
    "![Loss and Accuracy Percent](better1.png)\n",
    "\n",
    "![Loss and Accuracy Percent](better2.png)\n",
    "\n",
    "\n",
    "Histogram results of weight and biases over each iteration\n",
    "\n",
    "![Weights](weights_better.png)\n",
    "\n",
    "![Biases](biases_better.png)\n",
    "\n",
    "\n",
    "The final result after testing one batch is 99.21%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
