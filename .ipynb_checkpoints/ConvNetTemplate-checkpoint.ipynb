{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network for MNIST Dataset\n",
    "\n",
    "Before starting the model we first needed to define some parameters and load the MNIST dataset. N_CLASSES is the number of classes in our dataset (10 digits). The other variables that must be initilialized are all vairables that can be tuned further to better fit our model. THe given values are ones that have been tested for >95% testing accuracy with only N_EPOCHS = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Using convolutional net on MNIST dataset of handwritten digit\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)\n",
    "N_CLASSES = 10\n",
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 50\n",
    "SKIP_STEP = 100\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also need to create some variables for our model and our dataset. Each image in MNISt is represented by a 28x28 pixel, whihc will be represented by a 1x784 tensor in this model. \n",
    "\n",
    "We also initialize a dropout probablity which we will apply as a hidden layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
    "# for the dropout probability too\n",
    "# Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "# Step 4 + 5: create weights + do inference\n",
    "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "global_step = tf.Variable(\n",
    "    0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "utils.make_dir('checkpoints')\n",
    "utils.make_dir('checkpoints/convnet_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Convolutional Network Model\n",
    "\n",
    "As we see in our previous model, in order to improve upon Logistic Regression, in this example we will create a convolutional network that will involve two layers of convolution and pooling, a fully-connected RELU layer, a dropout layer, and then finally a layer for softmax regression.\n",
    "\n",
    "![Fig 1: Convolutional Network](images/change.png \"Convolutional Network\")\n",
    "\n",
    "### First Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    # use the dynamic dimension -1\n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    kernel = tf.get_variable(\n",
    "        'kernel', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    biases = tf.get_variable( \n",
    "        'biases', [32], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "\n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # conv1 = tf.layers.conv2d(x,32,5,activation=tf.nn.relu)\n",
    "\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "\n",
    "    conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "    # conv1 = tf.contrib.layers.conv2d(\n",
    "      #  images, 32, 5, 1, activation_fn=tf.nn.relu, padding='SAME')\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, we start with the implementation of our first layer, which will consist of convolution and then max pooling. The convolution will compute the first 32 features for each 5x5 patch. The weight tensor has dimensions [5,5,1,32], where the first two dimensions are the patch size, the following element is the number of input channels, and the last is the number of output channels. We will also have a bias vctor that aligns with our output channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[\n",
    "                           1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# output is of dimension BATCH_SIZE x 14 x 14 x 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling of our layer must occur in order to reduce the image size for our next layer with the size of Batch_SIZE x 13 x 13 x 32. \n",
    "\n",
    "### Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64],\n",
    "                             initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                             initializer=tf.constant_initializer(0.1))\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64\n",
    "    \n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, we will construct the second convotulional layer, with 64 features for each 5x5 patch that eventually gives us a outout of BATCH_SIZE x 7 x 7 64 after pooling. \n",
    "\n",
    "### Fully Connected Layer and Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * 64\n",
    "\n",
    "    # create weights and biases\n",
    "    w = tf.get_variable(\n",
    "        'weights', [input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable(\n",
    "        'biases', [1024], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "\n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our fully connected layer with all of our potential neurons sprocessing the entire image, we can reshape teh data into a batch of vectors and apply another ReLU. \n",
    "\n",
    "Dropout is used to reduce overfitting, according to the probability that a neuron's output is kept during dropout. This will also require us to only turn dropout on during training and off dring testing by modifying the DROPOUT_RATE \n",
    "\n",
    "### Specify Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # this you should know. get logits without softmax\n",
    "    # you need to create weights and biases\n",
    "\n",
    "    w = tf.get_variable(\n",
    "        'weights', [1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "    # Step 6: define loss function\n",
    "    # use softmax cross entropy with logits as the loss function\n",
    "    # compute mean cross entropy, softmax is applied internally\n",
    "with tf.name_scope('loss'):\n",
    "    # you should know how to do this too\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name='loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify the final layer which is a simply softmax regression that also inherently calculates our loss function that we want to minimize.\n",
    "\n",
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Step 7: define training op\n",
    "    # using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "    # don't forgot to pass in global_step\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    LEARNING_RATE).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our optimizer, we will replace the traditional gradient descent optimizer with the more sophisticated ADAM optimizer. We will also add additional TensorBoard summaries to be able to visiualize the performance of our model.\n",
    "\n",
    "tf.Session was used rather than tf.InteractiveSession since this better separates the process of creating the graph and the process of evaluating the graph. Using the with block, we can close the session immediately after the with block is exited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "\n",
    "    # You have to create folders to store checkpoints\n",
    "    ckpt = tf.train.get_checkpoint_state(\n",
    "        os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS):  # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, summary = sess.run([optimizer, loss, summary_op],\n",
    "                                          feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n",
    "        writer.add_summary(summary, global_step=index)\n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(\n",
    "                index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "\n",
    "    print(\"Optimization Finished!\")  # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is how we trained our model using the described optimier. We also added logging to every 100th iteration in the training process to understand how our training accuracy and average loss is going. DROPOUT is the parameter used to control the dropout rate. Here are the plots that were calculated from fully trained the model.\n",
    "\n",
    "\n",
    "![Fig 2: Training Accuracy](images/graph_acc.png)\n",
    "\n",
    "\n",
    "![Fig 3: Training Accuracy](images/summaries_acc.png)\n",
    "\n",
    "\n",
    "![Fig 4: Loss Function](images/graph_loss.png)\n",
    "\n",
    "\n",
    "![Fig 4: Loss Function](images/summaries_histo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples / BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        loss_batch, logits_batch = sess.run([loss, logits],\n",
    "                                            feed_dict={X: X_batch, Y: Y_batch, dropout: 1.0})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "    print(\"Accuracy {0}\".format(total_correct_preds / mnist.test.num_examples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the test data on our model after completing the optimization we end up with roughly 97.28% test accuracy, which is respectable with 25 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
